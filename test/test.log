[36mINFO[0m[2020-04-29T09:57:36-04:00] readConfig : Reading Config                  
[36mINFO[0m[2020-04-29T09:57:36-04:00] readConfig : ClusterName :  mylocalcluster   
[36mINFO[0m[2020-04-29T09:57:36-04:00] readConfig : SparkMasterURL :  http://localhost:8080 
[37mDEBU[0m[2020-04-29T09:57:41-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:57:41-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:57:41-04:00] initStandalone : activeApps 0                
[36mINFO[0m[2020-04-29T09:57:41-04:00] populateSparkMetrics : active applications: 0 
[36mINFO[0m[2020-04-29T09:57:41-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:57:46-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:57:46-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:57:46-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:57:46-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:57:46-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:57:46-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:57:46-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:57:46-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:57:46-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:57:46-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:57:46-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime: spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:48-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:57:48-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:57:48-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:57:51-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:57:51-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:57:51-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:57:51-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:57:51-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=64 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=64 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=64 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=64 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=796 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=7.44508077e+08 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=64 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalTasks=66 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalDuration=2757 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=60 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:51-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:57:51-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:57:51-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:57:56-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:57:56-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:57:56-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:57:56-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:57:56-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=417 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=417 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=417 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=417 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=3300 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=3.107786642e+09 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=419 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalTasks=421 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalDuration=8093 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=86 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:57:56-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:57:56-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:57:56-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:01-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:01-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:01-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:01-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:01-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=830 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=830 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=830 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=830 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=6152 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=5.821837621e+09 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=829 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalTasks=831 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalDuration=13331 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=97 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:01-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:01-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:01-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:06-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:06-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:06-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:06-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:06-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=1279 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=1279 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=1279 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=1279 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=9005 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=8.623999617e+09 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=1279 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalTasks=1281 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalDuration=18497 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=108 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:06-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:06-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:06-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:11-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:11-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:11-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:11-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:11-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=1751 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=1751 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=1751 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=1751 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=12051 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=1.1550886351e+10 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=1751 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalTasks=1753 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalDuration=23803 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=120 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:11-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:11-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:11-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:16-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:16-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:16-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:16-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:16-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=2256 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=2256 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=2256 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=2256 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=15139 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=1.4552529035e+10 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=2256 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalTasks=2258 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalDuration=29033 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=123 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:16-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:16-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:16-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:21-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:21-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:21-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:21-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:21-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=2751 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=2751 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=2751 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=2751 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=18254 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=1.7539844425e+10 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=2751 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalTasks=2753 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalDuration=34244 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=133 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:21-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:21-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:21-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:26-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:26-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:26-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:26-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:26-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=3278 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=3278 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=3278 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=3278 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=21433 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=2.061469912e+10 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=3278 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalTasks=3280 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalDuration=39474 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=142 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:26-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:26-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:26-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:31-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:31-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:31-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:31-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:31-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=3792 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=3792 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=3792 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=3792 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=24601 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=2.3670684879e+10 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=3790 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalTasks=3792 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalDuration=44621 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=151 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:31-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:31-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:31-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:36-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:36-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:36-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:36-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:36-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=4301 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=4301 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=4301 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=4301 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=27836 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=2.6848504706e+10 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=4298 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalTasks=4300 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalDuration=49795 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=155 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:36-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:36-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:36-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:41-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:41-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:41-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:41-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:41-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=4734 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=4734 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=4734 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=4734 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=31074 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=3.0012228846e+10 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=4733 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalTasks=4735 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalDuration=55022 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=159 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:41-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:41-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:41-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:46-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:46-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:46-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:46-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:46-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=5096 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=5096 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=5096 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=5096 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=34308 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=3.3108393488e+10 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=5096 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalTasks=5098 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalDuration=60203 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=172 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:46-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:46-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:46-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:51-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:51-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:51-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:51-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:51-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=5391 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=5391 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=5391 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=5391 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=37436 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=3.6104688615e+10 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=5397 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalTasks=5399 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalDuration=65526 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=181 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:51-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:51-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:51-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:58:56-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:58:56-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:58:56-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:58:56-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:58:56-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=5672 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=5672 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=5677 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=5677 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=40622 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=3.9104513417e+10 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=5677 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalTasks=5679 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalDuration=70734 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=189 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:58:56-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:58:56-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:58:56-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:59:01-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:59:01-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:59:01-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:59:01-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:59:01-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=5957 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=5957 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=5963 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=5963 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=43914 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=4.2191309421e+10 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=5963 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalTasks=5965 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalDuration=76011 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=208 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:01-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:59:01-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:59:01-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:59:06-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:59:06-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:59:06-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:59:06-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:59:06-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=6323 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=6323 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=6323 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=6323 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=47180 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=4.5346190839e+10 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=6323 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalTasks=6325 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalDuration=81086 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=226 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:06-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:59:06-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:59:06-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:59:11-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:59:11-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:59:11-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:59:11-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:59:11-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=6747 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=6747 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=6747 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=6747 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=50521 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=4.8547482685e+10 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=6747 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalTasks=6749 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalDuration=86313 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=231 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:11-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:59:11-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:59:11-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:59:16-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:59:16-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:59:16-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:59:16-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:59:16-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=1 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=7215 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=7215 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=1 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=7215 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=7215 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=53835 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=5.1727440881e+10 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=1 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=7215 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalTasks=7216 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalDuration=91468 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=235 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:16-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:59:16-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:59:16-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:59:21-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:59:21-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:59:21-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:59:21-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:59:21-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=7758 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=7758 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=7758 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=7758 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=57244 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=5.501617965e+10 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=7760 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalTasks=7762 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalDuration=96653 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=257 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:21-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:59:21-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:59:21-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:59:26-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:59:26-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:59:26-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:59:26-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:59:26-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=8337 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=8337 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=8337 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=8337 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=60607 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=5.8261660218e+10 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=8338 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalTasks=8340 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalDuration=101822 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=279 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:26-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:59:26-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:59:26-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:59:31-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:59:31-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:59:31-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:59:31-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:59:31-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=8955 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=8955 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=8955 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=8955 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=63942 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=6.1489515911e+10 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=8956 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalTasks=8958 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalDuration=107006 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=294 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:31-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:59:31-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:59:31-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:59:36-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:59:36-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] initStandalone : activeApps 1                
[37mDEBU[0m[2020-04-29T09:59:36-04:00] initStandalone : app ID: app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] initStandalone : app name: Spark Pi          
[37mDEBU[0m[2020-04-29T09:59:36-04:00] getStandaloneAppURL : fetching getStandaloneAppURL 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] makeRequest : request :http://localhost:8080/app/?appId=app-20200429135744-0003 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] getStandaloneAppURL : Application Detail UIhttp://localhost:4040 
[36mINFO[0m[2020-04-29T09:59:36-04:00] populateSparkMetrics : active applications: 1 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] populateSparkMetrics : adding universal tags: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root] 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/jobs/ 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] populateJobMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.job.Name:reduce at SparkPi.scala:38 spark.job.StageIds:0 spark.job.Status:RUNNING spark.job.SubmissionTime:2020-04-29T13:57:46.310GMT] 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.JobID=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.Name=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.StageIds=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.Status=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumCompletedTasks=9592 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumSkippedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumCompletedIndices=9592 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumActiveStages=1 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumCompletedStages=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumSkippedStages=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.job.NumFailedStages=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/stages 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] populateStageMetrics : setting metrics for stage:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.stage.FirstTaskLaunchedTime:2020-04-29T13:57:48.908GMT spark.stage.Name:reduce at SparkPi.scala:38 spark.stage.RddIds:1,0 spark.stage.SchedulingPool:default spark.stage.Status:ACTIVE spark.stage.SubmissionTime:2020-04-29T13:57:46.454GMT] 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.Status=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.StageID=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.AttemptID=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.NumTasks=10000 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.NumActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.NumCompleteTasks=9592 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.NumFailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.NumKilledTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.NumCompletedIndices=9592 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.ExecutorRunTime=67315 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.ExecutorCPUTime=6.4780304739e+10 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.SubmissionTime=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.FirstTaskLaunchedTime=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.InputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.InputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.OutputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.OutputRecords=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.ShuffleReadBytes=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.ShuffleReadRecords=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteBytes=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.ShuffleWriteRecords=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.MemoryBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.DiskBytesSpilled=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.Name=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.SchedulingPool=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.stage.RddIds=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/executors 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] populateExecutorMetrics : setting metrics for executor:driver, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:45.034GMT spark.executor.HostPort:localhost:34031 spark.executor.ID:driver] 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalCores=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.MaxTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalDuration=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.MaxMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=9.783214e+07 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] populateExecutorMetrics : setting metrics for executor:0, attributes: map[spark.app.Cores:1 spark.app.ID:app-20200429135744-0003 spark.app.Memoryperslave:1024 spark.app.Name:Spark Pi spark.app.Starttime:1588168664887 spark.app.State:RUNNING spark.app.Submitdate:Wed Apr 29 13:57:44 GMT 2020 spark.app.User:root spark.executor.AddTime:2020-04-29T13:57:48.902GMT spark.executor.HostPort:172.20.0.3:44921 spark.executor.ID:0] 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.ID=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.HostPort=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.IsActive=1 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.RddBlocks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.MemoryUsed=1381 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.DiskUsed=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalCores=1 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.MaxTasks=1 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.ActiveTasks=2 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.FailedTasks=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.CompletedTasks=9595 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalTasks=9597 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalDuration=112278 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalGCTime=318 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalInputBytes=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalShuffleRead=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalShuffleWrite=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.IsBlacklisted=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.MaxMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.AddTime=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.UsedOnHeapStorageMemory=1381 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.UsedOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalOnHeapStorageMemory=3.84093388e+08 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] setMetrics : recording metric: spark.executor.TotalOffHeapStorageMemory=0 
[37mDEBU[0m[2020-04-29T09:59:36-04:00] makeRequest : request :http://localhost:4040/api/v1/applications/app-20200429135744-0003/streaming/statistics 
[33mWARN[0m[2020-04-29T09:59:36-04:00] populateStreamingMetrics : error processing json: invalid character 'o' in literal null (expecting 'u') 
[36mINFO[0m[2020-04-29T09:59:36-04:00] harvesting metrics ..                        
[37mDEBU[0m[2020-04-29T09:59:41-04:00] initStandalone : querying masterUI           
[37mDEBU[0m[2020-04-29T09:59:41-04:00] makeRequest : request :http://localhost:8080/json/ 
[37mDEBU[0m[2020-04-29T09:59:41-04:00] initStandalone : activeApps 0                
[36mINFO[0m[2020-04-29T09:59:41-04:00] populateSparkMetrics : active applications: 0 
[36mINFO[0m[2020-04-29T09:59:41-04:00] harvesting metrics ..                        
